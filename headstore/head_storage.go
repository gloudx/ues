package headstore

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sync"

	"github.com/ipfs/go-cid"
	ds "github.com/ipfs/go-datastore"
)

// HeadStorage управляет персистентным хранением состояния HEAD репозитория.
// Интерфейс обеспечивает абстракцию над различными механизмами хранения (datastore, файловая система).
// Основная цель - автоматическое сохранение и восстановление состояния между перезапусками приложения,
// а также предоставление механизма подписки на изменения для репликации и синхронизации.
//
// Реализации должны обеспечивать:
// - Атомарность операций записи
// - Консистентность данных при concurrent доступе
// - Уведомления watchers об изменениях состояния
// - Корректную очистку ресурсов при закрытии
type HeadStorage interface {
	// LoadHead загружает последнее состояние HEAD из persistent storage.
	//
	// Параметры:
	//   ctx - контекст для отмены операции и передачи метаданных
	//   repoID - уникальный идентификатор репозитория
	//
	// Возвращает:
	//   RepositoryState - структуру с актуальным состоянием репозитория
	//   error - ошибку, если не удалось загрузить состояние
	//
	// Поведение:
	//   - Если репозиторий еще не инициализирован, возвращает состояние по умолчанию
	//   - При отсутствии данных (новый репозиторий) не считается ошибкой
	//   - Должен быть потокобезопасным для concurrent вызовов
	LoadHead(ctx context.Context, repoID string) (RepositoryState, error)

	// SaveHead сохраняет текущее состояние HEAD в persistent storage.
	//
	// Параметры:
	//   ctx - контекст для отмены операции и передачи метаданных
	//   repoID - уникальный идентификатор репозитория
	//   state - новое состояние репозитория для сохранения
	//
	// Возвращает:
	//   error - ошибку, если не удалось сохранить состояние
	//
	// Поведение:
	//   - Операция должна быть атомарной (либо полностью успешной, либо откатываться)
	//   - После успешного сохранения уведомляет всех watchers об изменении
	//   - Должен быть потокобезопасным для concurrent вызовов
	//   - Сериализует состояние в формат, подходящий для storage backend
	SaveHead(ctx context.Context, repoID string, state RepositoryState) error

	// WatchHead создает канал для подписки на изменения состояния HEAD репозитория.
	// Используется для реализации репликации, синхронизации и real-time уведомлений.
	//
	// Параметры:
	//   ctx - контекст, при отмене которого подписка автоматически закрывается
	//   repoID - уникальный идентификатор репозитория для отслеживания
	//
	// Возвращает:
	//   <-chan RepositoryState - только для чтения канал с уведомлениями об изменениях
	//   error - ошибку, если не удалось создать подписку
	//
	// Поведение:
	//   - Канал буферизованный для предотвращения блокировки при медленных consumer'ах
	//   - При отмене контекста канал автоматически закрывается
	//   - Не отправляет текущее состояние при подписке, только изменения
	//   - При переполнении буфера пропускает уведомления (non-blocking)
	WatchHead(ctx context.Context, repoID string) (<-chan RepositoryState, error)

	// Close корректно закрывает storage и освобождает все связанные ресурсы.
	//
	// Возвращает:
	//   error - ошибку, если не удалось корректно закрыть storage
	//
	// Поведение:
	//   - Закрывает все активные watchers и их каналы
	//   - Освобождает внутренние ресурсы (подключения, файлы, память)
	//   - После вызова Close все остальные методы могут возвращать ошибки
	//   - Должен быть idempotent (безопасен для повторного вызова)
	Close() error
}

// RepositoryState представляет полный снимок состояния репозитория в определенный момент времени.
// Структура содержит всю необходимую информацию для восстановления состояния репозитория
// после перезапуска приложения или для репликации на другие узлы.
//
// Все поля являются обязательными для корректной работы, кроме случаев инициализации нового репозитория.
// Структура сериализуется в JSON для хранения в различных storage backend'ах.
//
// Версионирование:
//
//	Version поле используется для миграций формата при изменении структуры
//	Текущая версия: 1
type RepositoryState struct {
	// Head содержит CID текущего HEAD коммита репозитория.
	// Это основная точка входа для навигации по истории коммитов.
	// При инициализации нового репозитория может быть cid.Undef.
	Head cid.Cid `json:"head"`

	// Prev содержит CID предыдущего коммита в цепочке.
	// Используется для быстрого построения истории и валидации целостности.
	// Для первого коммита в репозитории будет cid.Undef.
	Prev cid.Cid `json:"prev"`

	// RootIndex содержит CID корневого индекса для текущего состояния.
	// Кэшируется для улучшения производительности доступа к данным.
	// Позволяет избежать пересчета индекса при каждом обращении к данным.
	RootIndex cid.Cid `json:"root"`

	// Version указывает версию формата структуры RepositoryState.
	// Используется для обратной совместимости и миграций при изменении схемы.
	// Текущая версия: 1
	Version int `json:"version"`

	// RepoID содержит уникальный идентификатор репозитория.
	// Позволяет различать состояния разных репозиториев в shared storage.
	// Должен быть постоянным в течение всей жизни репозитория.
	RepoID string `json:"repo_id"`
}

// datastoreHeadStorage реализует интерфейс HeadStorage через IPFS datastore.
// Предоставляет надежное, распределенное хранение состояния репозитория
// с поддержкой concurrent доступа и уведомлений об изменениях.
//
// Архитектура:
//   - Использует иерархические ключи для организации данных
//   - Поддерживает множественные watchers для real-time уведомлений
//   - Обеспечивает потокобезопасность через RWMutex
//   - Автоматически управляет жизненным циклом watchers
//
// Формат ключей: /repository/{repoID}/head
// Формат данных: JSON сериализация RepositoryState
type datastoreHeadStorage struct {
	// ds - основное хранилище данных, реализующее интерфейс datastore
	// Обеспечивает персистентность и может быть любой совместимой реализацией
	// (файловой, in-memory, сетевой и т.д.)
	ds ds.Datastore

	// watchers содержит карту активных подписчиков для каждого репозитория
	// Ключ: repoID, Значение: слайс каналов для уведомлений
	// Используется для реализации pub/sub механизма
	watchers map[string][]chan RepositoryState

	// mu обеспечивает потокобезопасный доступ к watchers map
	// RWMutex позволяет concurrent чтение при exclusive записи
	mu sync.RWMutex
}

// NewDatastoreHeadStorage создает новый экземпляр HeadStorage на основе datastore.
// Конструктор инициализирует все внутренние структуры данных и возвращает
// готовый к использованию storage.
//
// Параметры:
//
//	store - реализация datastore для персистентного хранения
//	        Может быть любой совместимой реализацией (file, memory, network)
//
// Возвращает:
//
//	HeadStorage - готовый к использованию storage с инициализированными структурами
//
// Особенности:
//   - Не выполняет никаких I/O операций при создании
//   - Инициализирует пустую карту watchers
//   - Безопасен для concurrent использования сразу после создания
//   - Не проверяет доступность или корректность переданного store
func NewDatastoreHeadStorage(store ds.Datastore) HeadStorage {
	return &datastoreHeadStorage{
		ds:       store,
		watchers: make(map[string][]chan RepositoryState),
	}
}

// LoadHead загружает последнее сохраненное состояние репозитория из datastore.
// Метод выполняет десериализацию JSON данных в структуру RepositoryState.
//
// Алгоритм работы:
//  1. Формирует иерархический ключ для поиска данных репозитория
//  2. Выполняет запрос к datastore для получения сырых данных
//  3. Обрабатывает случай отсутствия данных (новый репозиторий)
//  4. Десериализует JSON в структуру RepositoryState
//  5. Возвращает готовое состояние или ошибку
//
// Параметры:
//
//	ctx - контекст для отмены операции и timeout'ов
//	repoID - уникальный идентификатор репозитория
//
// Возвращает:
//
//	RepositoryState - десериализованное состояние репозитория
//	error - ошибку загрузки или десериализации
//
// Обработка ошибок:
//   - ds.ErrNotFound: возвращает состояние по умолчанию для нового репозитория
//   - JSON unmarshal errors: возвращает ошибку с контекстом
//   - Datastore errors: оборачивает и возвращает с дополнительным контекстом
//
// Потокобезопасность:
//   - Метод полностью потокобезопасен
//   - Не модифицирует внутреннее состояние объекта
//   - Может выполняться concurrent с другими операциями
func (h *datastoreHeadStorage) LoadHead(ctx context.Context, repoID string) (RepositoryState, error) {
	// Формируем иерархический ключ: /repository/{repoID}/head
	// Такая структура позволяет легко масштабировать и организовывать данные
	key := ds.NewKey("repository").ChildString(repoID).ChildString("head")

	// Выполняем запрос к datastore с учетом контекста
	data, err := h.ds.Get(ctx, key)
	if err != nil {
		// Специальная обработка случая отсутствия данных
		// Для нового репозитория это нормальная ситуация, не ошибка
		if err == ds.ErrNotFound {
			// Возвращаем состояние по умолчанию для неинициализированного репозитория
			return RepositoryState{
				Head:    cid.Undef, // Undefined CID указывает на отсутствие коммитов
				Prev:    cid.Undef, // Нет предыдущего коммита
				Version: 1,         // Текущая версия формата
				RepoID:  repoID,    // Сохраняем переданный идентификатор
			}, nil
		}
		// Для всех остальных ошибок datastore оборачиваем с контекстом
		return RepositoryState{}, fmt.Errorf("failed to load head state: %w", err)
	}

	// Десериализуем JSON данные в структуру
	var state RepositoryState
	if err := json.Unmarshal(data, &state); err != nil {
		// Ошибки десериализации могут указывать на corruption данных
		// или несовместимость версий формата
		return RepositoryState{}, fmt.Errorf("failed to unmarshal head state: %w", err)
	}

	return state, nil
}

// SaveHead сохраняет текущее состояние репозитория в datastore и уведомляет watchers.
// Метод обеспечивает атомарность операции и автоматическое уведомление подписчиков.
//
// Алгоритм работы:
//  1. Сериализует состояние в JSON формат
//  2. Формирует ключ для хранения в datastore
//  3. Атомарно сохраняет данные в datastore
//  4. При успешном сохранении уведомляет всех активных watchers
//
// Параметры:
//
//	ctx - контекст для отмены операции и timeout'ов
//	repoID - уникальный идентификатор репозитория
//	state - новое состояние репозитория для сохранения
//
// Возвращает:
//
//	error - ошибку сериализации или сохранения
//
// Гарантии:
//   - Операция атомарна: либо полностью успешна, либо не выполняется
//   - При ошибке сохранения watchers не уведомляются
//   - JSON сериализация выполняется с компактным форматом
//   - Уведомления watchers происходят только после успешного сохранения
//
// Потокобезопасность:
//   - Полностью потокобезопасен для concurrent вызовов
//   - Уведомления watchers защищены от race conditions
//   - Не блокируется на медленных watchers благодаря non-blocking отправке
//
// Обработка ошибок:
//   - JSON marshal errors: проблемы с сериализацией структуры
//   - Datastore errors: проблемы с underlying storage
//   - Все ошибки оборачиваются с контекстом для лучшей диагностики
func (h *datastoreHeadStorage) SaveHead(ctx context.Context, repoID string, state RepositoryState) error {
	// Формируем тот же иерархический ключ, что и при загрузке
	key := ds.NewKey("repository").ChildString(repoID).ChildString("head")

	// Сериализуем состояние в компактный JSON
	// Используем стандартную сериализацию без отступов для экономии места
	data, err := json.Marshal(state)
	if err != nil {
		// Ошибки сериализации обычно указывают на проблемы со структурой данных
		return fmt.Errorf("failed to marshal head state: %w", err)
	}

	// Атомарно сохраняем данные в datastore
	// Datastore должен гарантировать atomicity этой операции
	if err := h.ds.Put(ctx, key, data); err != nil {
		// Оборачиваем ошибку datastore с контекстом для лучшей диагностики
		return fmt.Errorf("failed to save head state: %w", err)
	}

	// Уведомляем всех подписчиков об изменении состояния
	// Это происходит только после успешного сохранения
	// Уведомления отправляются асинхронно и не блокируют операцию сохранения
	h.notifyWatchers(repoID, state)

	return nil
}

// WatchHead создает новую подписку на изменения состояния HEAD указанного репозитория.
// Возвращает буферизованный канал, через который будут приходить уведомления
// о каждом изменении состояния репозитория.
//
// Алгоритм работы:
//  1. Создает буферизованный канал для уведомлений
//  2. Добавляет канал в список активных watchers для репозитория
//  3. Запускает горутину для автоматической очистки при отмене контекста
//  4. Возвращает read-only канал клиенту
//
// Параметры:
//
//	ctx - контекст, при отмене которого подписка автоматически закрывается
//	repoID - уникальный идентификатор репозитория для отслеживания
//
// Возвращает:
//
//	<-chan RepositoryState - read-only канал для получения уведомлений
//	error - ошибку создания подписки (в текущей реализации всегда nil)
//
// Особенности буферизации:
//   - Канал буферизован на 10 элементов для предотвращения блокировки
//   - При переполнении буфера новые уведомления пропускаются (non-blocking)
//   - Это защищает систему от медленных или зависших consumer'ов
//
// Управление жизненным циклом:
//   - При отмене контекста канал автоматически закрывается
//   - Watcher автоматически удаляется из внутреннего реестра
//   - Горутина автоматически завершается при закрытии канала
//   - Нет необходимости в ручной очистке ресурсов
//
// Потокобезопасность:
//   - Полностью потокобезопасен для concurrent подписок
//   - Защищен мьютексом от race conditions при добавлении watchers
//   - Безопасен для использования из множественных горутин
//
// Примечания по производительности:
//   - Подписка создается мгновенно без I/O операций
//   - Не отправляет текущее состояние при создании подписки
//   - Уведомления отправляются только при изменениях через SaveHead
func (h *datastoreHeadStorage) WatchHead(ctx context.Context, repoID string) (<-chan RepositoryState, error) {
	// Создаем буферизованный канал для предотвращения блокировки при медленных consumer'ах
	// Размер буфера 10 - компромисс между памятью и защитой от кратковременных задержек
	ch := make(chan RepositoryState, 10)

	// Безопасно добавляем новый watcher в реестр
	h.mu.Lock()
	h.watchers[repoID] = append(h.watchers[repoID], ch)
	h.mu.Unlock()

	// Запускаем горутину для автоматической очистки при отмене контекста
	// Это обеспечивает правильное управление ресурсами без участия клиента
	go func() {
		// Ожидаем отмены контекста
		<-ctx.Done()

		// Удаляем watcher из реестра для предотвращения утечек памяти
		h.removeWatcher(repoID, ch)

		// Закрываем канал для уведомления consumer'а о завершении подписки
		close(ch)
	}()

	// Возвращаем read-only канал клиенту
	return ch, nil
}

// Close выполняет корректное закрытие datastoreHeadStorage и освобождение всех ресурсов.
// Метод обеспечивает graceful shutdown всех активных подписок и очистку внутреннего состояния.
//
// Алгоритм работы:
//  1. Блокирует доступ к watchers для предотвращения новых подписок
//  2. Итерирует по всем активным watchers для всех репозиториев
//  3. Закрывает каждый канал для уведомления consumer'ов о завершении
//  4. Очищает внутреннюю карту watchers для предотвращения утечек памяти
//  5. Освобождает блокировку
//
// Возвращает:
//
//	error - ошибку закрытия (в текущей реализации всегда nil)
//
// Поведение после закрытия:
//   - Все активные подписки (WatchHead) получат закрытые каналы
//   - Новые вызовы WatchHead могут создавать watchers, но это не рекомендуется
//   - SaveHead и LoadHead остаются функциональными (зависят от datastore)
//   - Повторный вызов Close безопасен (idempotent)
//
// Потокобезопасность:
//   - Использует exclusive lock для полной синхронизации
//   - Предотвращает race conditions при concurrent закрытии
//   - Гарантирует, что все watchers будут корректно закрыты
//
// Управление ресурсами:
//   - Не закрывает underlying datastore (не является владельцем)
//   - Закрывает только каналы, созданные этим storage
//   - Очищает всю внутреннюю память, связанную с watchers
//
// Примечания:
//   - Метод блокирующий и может занять время при большом количестве watchers
//   - Рекомендуется вызывать в defer или explicit cleanup
//   - После вызова объект может быть повторно использован, но это не рекомендуется
func (h *datastoreHeadStorage) Close() error {
	// Получаем exclusive lock для предотвращения concurrent доступа
	// во время закрытия ресурсов
	h.mu.Lock()
	defer h.mu.Unlock()

	// Итерируем по всем репозиториям и их watchers
	for _, watchers := range h.watchers {
		// Закрываем каждый канал для уведомления consumer'ов
		for _, ch := range watchers {
			// close() безопасен для уже закрытых каналов,
			// но мы не проверяем это для упрощения кода
			close(ch)
		}
	}

	// Создаем новую пустую карту для очистки ссылок на старые каналы
	// Это важно для предотвращения утечек памяти
	h.watchers = make(map[string][]chan RepositoryState)

	// В текущей реализации всегда возвращаем nil
	// В будущем здесь может быть логика закрытия дополнительных ресурсов
	return nil
}

// notifyWatchers отправляет уведомления о изменении состояния всем активным подписчикам.
// Метод реализует non-blocking уведомления для предотвращения deadlocks и зависаний.
//
// Алгоритм работы:
//  1. Получает read lock для безопасного доступа к списку watchers
//  2. Создает локальную копию списка watchers для указанного репозитория
//  3. Освобождает lock для минимизации времени блокировки
//  4. Итерирует по всем watchers и пытается отправить уведомление
//  5. Использует select с default для non-blocking отправки
//
// Параметры:
//
//	repoID - идентификатор репозитория, watchers которого нужно уведомить
//	state - новое состояние репозитория для отправки
//
// Non-blocking гарантии:
//   - Если канал watcher'а заполнен, уведомление пропускается
//   - Это предотвращает блокировку операций SaveHead на медленных consumer'ах
//   - Медленные или зависшие watchers не влияют на производительность системы
//
// Потокобезопасность:
//   - Использует RWMutex для concurrent доступа к watchers
//   - Read lock позволяет множественным уведомлениям выполняться параллельно
//   - Минимизирует время блокировки для лучшей производительности
//
// Обработка закрытых каналов:
//   - Отправка в закрытый канал вызовет panic
//   - В текущей реализации предполагается, что каналы управляются корректно
//   - Close() должен вызываться перед удалением watchers
//
// Примечания по производительности:
//   - Быстрое выполнение благодаря non-blocking отправке
//   - Не выполняет I/O операций или тяжелых вычислений
//   - Минимальное время удержания locks для лучшего concurrency
func (h *datastoreHeadStorage) notifyWatchers(repoID string, state RepositoryState) {
	// Получаем read lock для безопасного доступа к watchers map
	// RLock позволяет concurrent выполнение множественных уведомлений
	h.mu.RLock()
	// Создаем локальную копию слайса watchers для минимизации времени lock'а
	watchers := h.watchers[repoID]
	h.mu.RUnlock()

	// Итерируем по всем watchers для данного репозитория
	for _, ch := range watchers {
		// Используем select с default для non-blocking отправки
		select {
		case ch <- state:
			// Уведомление успешно отправлено в канал
			// Consumer получит новое состояние репозитория
		default:
			// Канал заполнен или watcher медленный - пропускаем уведомление
			// Это предотвращает блокировку всей системы из-за одного медленного consumer'а
			// В production может быть полезно логировать такие случаи для мониторинга
		}
	}
}

// removeWatcher безопасно удаляет указанный watcher из списка активных подписчиков.
// Метод используется для очистки ресурсов при отмене контекста или закрытии подписки.
//
// Алгоритм работы:
//  1. Получает exclusive lock для безопасной модификации watchers map
//  2. Находит указанный канал в списке watchers для репозитория
//  3. Удаляет найденный канал из слайса, сохраняя порядок остальных
//  4. Обновляет watchers map с новым слайсом
//  5. Освобождает lock
//
// Параметры:
//
//	repoID - идентификатор репозитория, из watchers которого удаляется подписчик
//	target - конкретный канал, который нужно удалить из списка
//
// Поиск и удаление:
//   - Использует линейный поиск по адресу канала (pointer comparison)
//   - Удаляет только первое найденное совпадение
//   - Если канал не найден, операция завершается без изменений
//   - Использует эффективное slice manipulation для удаления элемента
//
// Потокобезопасность:
//   - Использует exclusive lock для предотвращения race conditions
//   - Безопасен для concurrent вызовов из разных горутин
//   - Синхронизирован с notifyWatchers и WatchHead методами
//
// Управление памятью:
//   - Правильное удаление предотвращает утечки памяти
//   - Не закрывает канал (это ответственность вызывающего кода)
//   - Уменьшает slice capacity при необходимости
//
// Производительность:
//   - O(n) сложность по количеству watchers для репозитория
//   - Быстрое выполнение благодаря простой логике поиска
//   - Минимальное время удержания lock'а
//
// Примечания:
//   - Метод не проверяет, закрыт ли канал
//   - Предполагается, что target канал существует в списке
//   - Если канал отсутствует, операция молча завершается
func (h *datastoreHeadStorage) removeWatcher(repoID string, target chan RepositoryState) {
	// Получаем exclusive lock для безопасной модификации watchers map
	h.mu.Lock()
	defer h.mu.Unlock()

	// Получаем текущий список watchers для репозитория
	watchers := h.watchers[repoID]

	// Выполняем линейный поиск целевого канала
	for i, ch := range watchers {
		// Сравниваем по адресу канала (pointer comparison)
		if ch == target {
			// Удаляем элемент из слайса, сохраняя порядок
			// Используем стандартный Go паттерн для удаления элемента из slice
			h.watchers[repoID] = append(watchers[:i], watchers[i+1:]...)

			// Прерываем поиск после первого найденного совпадения
			// Это предотвращает удаление дубликатов, если они есть
			break
		}
	}
}

// fileHeadStorage реализует интерфейс HeadStorage через файловую систему.
// Предоставляет простое и надежное хранение состояния репозитория в локальных файлах.
// Подходит для single-node развертываний и случаев, когда не требуется распределенное хранение.
//
// Архитектура:
//   - Каждый репозиторий хранится в отдельном JSON файле
//   - Использует атомарную запись через временные файлы
//   - Поддерживает watchers аналогично datastoreHeadStorage
//   - Автоматически создает директории при необходимости
//
// Структура файлов:
//
//	{baseDir}/{repoID}.json - основной файл состояния
//	{baseDir}/{repoID}.json.tmp - временный файл для атомарной записи
//
// Преимущества:
//   - Простота реализации и отладки
//   - Человекочитаемый формат хранения (JSON с отступами)
//   - Не требует внешних зависимостей
//   - Легкое резервное копирование и восстановление
//
// Ограничения:
//   - Не подходит для distributed систем
//   - Ограниченная производительность при большом количестве репозиториев
//   - Нет встроенной репликации или консистентности между узлами
type fileHeadStorage struct {
	// baseDir - базовая директория для хранения файлов состояния
	// Все файлы репозиториев будут создаваться в этой директории
	// Должна иметь права на чтение и запись для текущего пользователя
	baseDir string

	// watchers содержит карту активных подписчиков для каждого репозитория
	// Ключ: repoID, Значение: слайс каналов для уведомлений
	// Идентична по структуре datastoreHeadStorage для консистентности API
	watchers map[string][]chan RepositoryState

	// mu обеспечивает потокобезопасный доступ к watchers map
	// RWMutex позволяет concurrent чтение при exclusive записи
	mu sync.RWMutex
}

// NewFileHeadStorage создает новый экземпляр HeadStorage на основе файловой системы.
// Конструктор проверяет доступность директории и создает её при необходимости.
//
// Алгоритм работы:
//  1. Проверяет существование базовой директории
//  2. Создает директорию с правами 0755 при её отсутствии
//  3. Инициализирует внутренние структуры данных
//  4. Возвращает готовый к использованию storage
//
// Параметры:
//
//	baseDir - путь к директории для хранения файлов состояния
//	          Может быть относительным или абсолютным путем
//	          Директория будет создана, если не существует
//
// Возвращает:
//
//	HeadStorage - готовый к использованию файловый storage
//	error - ошибку создания директории или проверки прав доступа
//
// Права доступа:
//   - Созданная директория получает права 0755 (rwxr-xr-x)
//   - Владелец: полные права (чтение, запись, выполнение)
//   - Группа и остальные: только чтение и выполнение
//
// Проверки при создании:
//   - Существование и доступность родительских директорий
//   - Права на создание новых файлов в указанной директории
//   - Корректность пути (не является файлом)
//
// Обработка ошибок:
//   - Ошибки файловой системы (нет прав, диск полон, и т.д.)
//   - Некорректные пути (например, когда путь указывает на существующий файл)
//   - Системные ограничения (максимальная длина пути и т.д.)
//
// Примечания:
//   - Не выполняет валидацию содержимого существующих файлов
//   - Не очищает директорию от старых или некорректных файлов
//   - Не проверяет доступность места на диске
func NewFileHeadStorage(baseDir string) (HeadStorage, error) {
	// Создаем базовую директорию с правами 0755, включая все родительские директории
	// MkdirAll аналогичен команде mkdir -p и безопасен при существующей директории
	if err := os.MkdirAll(baseDir, 0755); err != nil {
		// Оборачиваем ошибку с контекстом для лучшей диагностики проблем
		return nil, fmt.Errorf("failed to create base directory: %w", err)
	}

	// Создаем и возвращаем новый экземпляр с инициализированными структурами
	return &fileHeadStorage{
		baseDir:  baseDir,                                 // Сохраняем путь для последующих операций
		watchers: make(map[string][]chan RepositoryState), // Инициализируем пустую карту watchers
	}, nil
}

// LoadHead загружает состояние репозитория из JSON файла в файловой системе.
// Метод читает файл и десериализует JSON данные в структуру RepositoryState.
//
// Алгоритм работы:
//  1. Формирует путь к файлу состояния на основе repoID
//  2. Читает содержимое файла целиком в память
//  3. Обрабатывает случай отсутствия файла (новый репозиторий)
//  4. Десериализует JSON в структуру RepositoryState
//  5. Возвращает состояние или ошибку
//
// Параметры:
//
//	ctx - контекст для отмены операции (в файловой реализации не используется напрямую)
//	repoID - уникальный идентификатор репозитория
//
// Возвращает:
//
//	RepositoryState - загруженное состояние репозитория
//	error - ошибку чтения файла или десериализации
//
// Формат файла:
//   - Имя файла: {repoID}.json
//   - Содержимое: JSON представление RepositoryState
//   - Кодировка: UTF-8
//   - Может содержать отступы для удобства чтения
//
// Обработка отсутствующих файлов:
//   - os.IsNotExist(err) используется для определения отсутствия файла
//   - Возвращается состояние по умолчанию для нового репозитория
//   - Не считается ошибкой - нормальное поведение для первого запуска
//
// Обработка ошибок:
//   - Ошибки чтения файла: права доступа, I/O ошибки, отсутствие диска
//   - Ошибки десериализации: некорректный JSON, несовместимые типы
//   - Все ошибки оборачиваются с контекстом для диагностики
//
// Производительность:
//   - Читает файл целиком в память (подходит для небольших состояний)
//   - Нет кэширования - каждый вызов читает с диска
//   - Блокирующие I/O операции могут влиять на производительность
//
// Потокобезопасность:
//   - Операция чтения потокобезопасна на уровне ОС
//   - Не модифицирует внутреннее состояние объекта
//   - Может выполняться concurrent с другими операциями чтения
func (f *fileHeadStorage) LoadHead(ctx context.Context, repoID string) (RepositoryState, error) {
	// Формируем полный путь к файлу состояния
	// Используем filepath.Join для корректной обработки разделителей путей в разных ОС
	filePath := filepath.Join(f.baseDir, repoID+".json")

	// Читаем содержимое файла целиком в память
	data, err := os.ReadFile(filePath)
	if err != nil {
		// Специальная обработка случая отсутствия файла
		if os.IsNotExist(err) {
			// Файл не существует - это нормально для нового репозитория
			// Возвращаем состояние по умолчанию
			return RepositoryState{
				Head:    cid.Undef, // Undefined CID для нового репозитория
				Prev:    cid.Undef, // Нет предыдущего коммита
				Version: 1,         // Текущая версия формата
				RepoID:  repoID,    // Сохраняем переданный идентификатор
			}, nil
		}
		// Для всех остальных ошибок чтения файла
		return RepositoryState{}, fmt.Errorf("failed to read head file: %w", err)
	}

	// Десериализуем JSON данные в структуру
	var state RepositoryState
	if err := json.Unmarshal(data, &state); err != nil {
		// Ошибки десериализации могут указывать на corruption файла
		// или несовместимость версий формата
		return RepositoryState{}, fmt.Errorf("failed to unmarshal head state: %w", err)
	}

	return state, nil
}

// SaveHead сохраняет состояние репозитория в JSON файл с использованием атомарной записи.
// Метод обеспечивает атомарность операции через запись во временный файл с последующим переименованием.
//
// Алгоритм атомарной записи:
//  1. Сериализует состояние в JSON с отступами для удобства чтения
//  2. Записывает данные во временный файл (.tmp суффикс)
//  3. После успешной записи переименовывает временный файл в основной
//  4. При ошибках очищает временный файл
//  5. Уведомляет watchers только после успешного сохранения
//
// Параметры:
//
//	ctx - контекст для отмены операции (в файловой реализации не используется напрямую)
//	repoID - уникальный идентификатор репозитория
//	state - состояние репозитория для сохранения
//
// Возвращает:
//
//	error - ошибку сериализации, записи файла или переименования
//
// Атомарность:
//   - Операция либо полностью успешна, либо не изменяет существующий файл
//   - Временный файл предотвращает partial writes и corruption
//   - os.Rename() является атомарной операцией на большинстве файловых систем
//   - При сбое питания или краше процесса основной файл остается неповрежденным
//
// Форматирование JSON:
//   - Использует json.MarshalIndent для человекочитаемого формата
//   - Отступы в 2 пробела для лучшей читаемости
//   - Увеличивает размер файла, но упрощает отладку и ручное редактирование
//
// Права доступа:
//   - Временный файл создается с правами 0644 (rw-r--r--)
//   - Владелец: чтение и запись
//   - Группа и остальные: только чтение
//   - Наследует права родительской директории
//
// Обработка ошибок:
//   - Ошибки сериализации: проблемы с JSON marshal
//   - Ошибки записи: нет места на диске, права доступа, I/O ошибки
//   - Ошибки переименования: файловая система, concurrent доступ
//   - Автоматическая очистка временного файла при ошибках
//
// Потокобезопасность:
//   - Безопасен для concurrent записи разных репозиториев
//   - Concurrent запись одного репозитория может вызвать race conditions
//   - Рекомендуется external синхронизация для одного репозитория
//
// Производительность:
//   - Записывает файл полностью при каждом изменении
//   - Подходит для нечастых обновлений состояния
//   - Может быть медленным при частых изменениях или больших состояниях
func (f *fileHeadStorage) SaveHead(ctx context.Context, repoID string, state RepositoryState) error {
	// Формируем пути для основного и временного файлов
	filePath := filepath.Join(f.baseDir, repoID+".json")
	tempPath := filePath + ".tmp"

	// Сериализуем состояние в JSON с отступами для читаемости
	// Используем 2 пробела для отступов как стандарт Go
	data, err := json.MarshalIndent(state, "", "  ")
	if err != nil {
		// Ошибки сериализации обычно указывают на проблемы со структурой данных
		return fmt.Errorf("failed to marshal head state: %w", err)
	}

	// Записываем данные во временный файл с правами 0644
	// Это первый этап атомарной записи
	if err := os.WriteFile(tempPath, data, 0644); err != nil {
		// Оборачиваем ошибку записи с контекстом
		return fmt.Errorf("failed to write temp file: %w", err)
	}

	// Атомарно переименовываем временный файл в основной
	// Это ключевая операция, обеспечивающая атомарность
	if err := os.Rename(tempPath, filePath); err != nil {
		// При ошибке переименования очищаем временный файл
		// Используем defer для гарантированной очистки даже при panic
		os.Remove(tempPath)
		return fmt.Errorf("failed to rename temp file: %w", err)
	}

	// Уведомляем watchers об успешном изменении состояния
	// Это происходит только после полного успешного сохранения
	f.notifyWatchers(repoID, state)

	return nil
}

// WatchHead создает подписку на изменения состояния HEAD для файлового storage.
// Реализация идентична datastoreHeadStorage, обеспечивая консистентный API.
//
// Особенности файловой реализации:
//   - Не отслеживает изменения файла на диске от внешних процессов
//   - Уведомления отправляются только при изменениях через SaveHead
//   - Подходит для single-process scenarios
//   - Может быть расширена с файловыми watchers (fsnotify) в будущем
//
// Алгоритм работы:
//  1. Создает буферизованный канал для уведомлений
//  2. Регистрирует канал в списке watchers для репозитория
//  3. Запускает горутину для автоматической очистки при отмене контекста
//  4. Возвращает read-only канал клиенту
//
// Параметры:
//
//	ctx - контекст, при отмене которого подписка автоматически закрывается
//	repoID - уникальный идентификатор репозитория для отслеживания
//
// Возвращает:
//
//	<-chan RepositoryState - read-only канал для получения уведомлений
//	error - ошибку создания подписки (всегда nil в текущей реализации)
//
// Ограничения файловой реализации:
//   - Не отслеживает прямые изменения файлов на диске
//   - Уведомления только от SaveHead текущего процесса
//   - Для multi-process scenarios нужна реализация с file system events
//
// Поведение:
//   - Идентично datastoreHeadStorage.WatchHead
//   - Буферизованный канал на 10 элементов
//   - Non-blocking уведомления при переполнении буфера
//   - Автоматическая очистка при отмене контекста
//
// Будущие улучшения:
//   - Интеграция с fsnotify для отслеживания файловых изменений
//   - Поддержка multi-process уведомлений
//   - Дополнительная фильтрация событий файловой системы
func (f *fileHeadStorage) WatchHead(ctx context.Context, repoID string) (<-chan RepositoryState, error) {
	// Создаем буферизованный канал аналогично datastoreHeadStorage
	ch := make(chan RepositoryState, 10)

	// Безопасно добавляем watcher в реестр
	f.mu.Lock()
	f.watchers[repoID] = append(f.watchers[repoID], ch)
	f.mu.Unlock()

	// Запускаем горутину для автоматической очистки при отмене контекста
	go func() {
		// Ожидаем отмены контекста
		<-ctx.Done()

		// Удаляем watcher из реестра
		f.removeWatcher(repoID, ch)

		// Закрываем канал для уведомления consumer'а
		close(ch)
	}()

	return ch, nil
}

// Close выполняет корректное закрытие fileHeadStorage и освобождение всех ресурсов.
// Реализация идентична datastoreHeadStorage для консистентности API.
//
// Алгоритм работы:
//  1. Получает exclusive lock для предотвращения concurrent доступа
//  2. Закрывает все активные каналы watchers для всех репозиториев
//  3. Очищает внутреннюю карту watchers для предотвращения утечек памяти
//  4. Освобождает блокировку
//
// Возвращает:
//
//	error - ошибку закрытия (всегда nil в текущей реализации)
//
// Управление ресурсами:
//   - Закрывает только каналы, созданные этим storage
//   - Не удаляет файлы состояния с диска (они остаются персистентными)
//   - Не изменяет права доступа к файлам или директориям
//   - Очищает только in-memory структуры
//
// Поведение после закрытия:
//   - Все активные WatchHead подписки получат закрытые каналы
//   - LoadHead и SaveHead остаются функциональными (работают с файлами)
//   - Новые WatchHead вызовы технически возможны, но не рекомендуются
//   - Повторный вызов Close безопасен (idempotent)
//
// Отличия от datastoreHeadStorage:
//   - Не закрывает файловые дескрипторы (они управляются ОС)
//   - Не требует закрытия underlying storage (файловая система)
//   - Файлы состояния остаются доступными после закрытия
//
// Примечания:
//   - Операция быстрая, поскольку работает только с памятью
//   - Не выполняет I/O операций с файловой системой
//   - Безопасна для вызова из defer или cleanup handlers
func (f *fileHeadStorage) Close() error {
	// Получаем exclusive lock для безопасного доступа ко всем watchers
	f.mu.Lock()
	defer f.mu.Unlock()

	// Итерируем по всем репозиториям и их watchers
	for _, watchers := range f.watchers {
		// Закрываем каждый канал для уведомления consumer'ов о завершении
		for _, ch := range watchers {
			close(ch)
		}
	}

	// Создаем новую пустую карту для очистки ссылок на закрытые каналы
	// Это предотвращает утечки памяти и позволяет GC очистить старые данные
	f.watchers = make(map[string][]chan RepositoryState)

	return nil
}

// notifyWatchers отправляет уведомления о изменении состояния всем подписчикам файлового storage.
// Реализация полностью идентична datastoreHeadStorage.notifyWatchers для консистентности API.
//
// Особенности файловой реализации:
//   - Уведомления отправляются только при изменениях через SaveHead
//   - Не реагирует на прямые изменения файлов на диске от внешних процессов
//   - Обеспечивает такое же поведение, как и datastore реализация
//
// Алгоритм работы:
//  1. Получает read lock для безопасного доступа к watchers
//  2. Создает локальную копию списка watchers для репозитория
//  3. Освобождает lock для минимизации времени блокировки
//  4. Отправляет non-blocking уведомления всем watchers
//
// Параметры:
//
//	repoID - идентификатор репозитория, watchers которого нужно уведомить
//	state - новое состояние репозитория для отправки
//
// Поведение:
//   - Идентично datastoreHeadStorage.notifyWatchers
//   - Non-blocking отправка для предотвращения deadlocks
//   - Пропуск уведомлений при переполненных каналах
//   - Минимальное время удержания locks
//
// Согласованность с datastore реализацией:
//   - Одинаковая логика обработки watchers
//   - Идентичные гарантии производительности
//   - Консистентное поведение API между реализациями
//
// Будущие улучшения:
//   - Может быть дополнена отслеживанием файловых событий
//   - Потенциальная интеграция с file system watchers
//   - Расширенная фильтрация уведомлений
func (f *fileHeadStorage) notifyWatchers(repoID string, state RepositoryState) {
	// Получаем read lock для безопасного доступа к watchers map
	f.mu.RLock()
	// Создаем локальную копию слайса watchers
	watchers := f.watchers[repoID]
	f.mu.RUnlock()

	// Итерируем по всем watchers для данного репозитория
	for _, ch := range watchers {
		// Используем select с default для non-blocking отправки
		select {
		case ch <- state:
			// Уведомление успешно отправлено
		default:
			// Канал заполнен - пропускаем уведомление для предотвращения блокировки
		}
	}
}

// removeWatcher безопасно удаляет указанный watcher из списка подписчиков файлового storage.
// Реализация полностью идентична datastoreHeadStorage.removeWatcher.
//
// Алгоритм работы:
//  1. Получает exclusive lock для безопасной модификации watchers
//  2. Находит целевой канал в списке watchers репозитория
//  3. Удаляет найденный канал, сохраняя порядок остальных
//  4. Обновляет watchers map и освобождает lock
//
// Параметры:
//
//	repoID - идентификатор репозитория, из watchers которого удаляется подписчик
//	target - конкретный канал для удаления из списка
//
// Поведение:
//   - Идентично datastoreHeadStorage.removeWatcher
//   - Линейный поиск по pointer equality
//   - Удаление только первого найденного совпадения
//   - Безопасная обработка отсутствующих каналов
//
// Согласованность с datastore реализацией:
//   - Одинаковая логика поиска и удаления
//   - Идентичная обработка edge cases
//   - Консистентное управление памятью
func (f *fileHeadStorage) removeWatcher(repoID string, target chan RepositoryState) {
	// Получаем exclusive lock для безопасной модификации watchers map
	f.mu.Lock()
	defer f.mu.Unlock()

	// Получаем текущий список watchers для репозитория
	watchers := f.watchers[repoID]

	// Выполняем линейный поиск целевого канала
	for i, ch := range watchers {
		// Сравниваем по адресу канала (pointer equality)
		if ch == target {
			// Удаляем элемент из слайса с сохранением порядка
			f.watchers[repoID] = append(watchers[:i], watchers[i+1:]...)
			break
		}
	}
}
